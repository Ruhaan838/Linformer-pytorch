<div align="center">

# Linformer: Transformer with Linear Complexity

<div style="display: flex; justify-content: center; gap: 10px;">

[![Medium](https://img.shields.io/badge/Medium-%23000000.svg?logo=medium&logoColor=white)](#)
[![Hugging Face](https://img.shields.io/badge/Hugging%20Face-FFD21E?logo=huggingface&logoColor=000)](#)
[![X](https://img.shields.io/badge/X-%23000000.svg?logo=X&logoColor=white)](#)
[![Kaggle](https://img.shields.io/badge/Kaggle-white?logo=kaggle)](#)

</div>

</div>

## ðŸ‘‹ Inroduction
So, Our Tradinal Transformer that Prposed in Attention is all you need paper it's solve the biggest problem of RNN (Recurrent Neural Network) is to gradient vanishing, but the Transformer requires too much amount of time and computation to train and evaluate. 

## ðŸª« Power Hunger: Transformer
